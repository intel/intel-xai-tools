{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b65f0c82",
   "metadata": {},
   "source": [
    "# Explaining Fine Tuned Text Classifier with PyTorch using the Intel® Explainable AI API\n",
    "\n",
    "This notebook demonstrates fine tuning pretrained models from [Hugging Face](https://huggingface.co) using text classification datasets from the [Hugging Face Datasets catalog](https://huggingface.co/datasets) or a custom dataset. The notebook uses [Intel® Extension for PyTorch*](https://github.com/intel/intel-extension-for-pytorch), which extends PyTorch with optimizations for an extra performance boost on Intel hardware.\n",
    "\n",
    "Please install the dependencies from the [pytorch_requirements.txt](/notebooks/pytorch_requirements.txt) file before executing this notebook.\n",
    "\n",
    "The notebook performs the following steps:\n",
    "1. [Import dependencies and setup parameters](#1.-Import-dependencies-and-setup-parameters)\n",
    "2. [Prepare the dataset](#2.-Prepare-the-dataset)\n",
    "3. [Prepare the Model for Fine Tuning and Evaluation](#3.-Prepare-the-Model-for-Fine-Tuning-and-Evaluation)\n",
    "4. [Export the model](#4.-Export-the-model)\n",
    "5. [Reload the model and make predictions](#5.-Reload-the-model-and-make-predictions)\n",
    "6. [Get Explainations with Intel Explainable AI Tools](#6.-Get-Explainations-with-Intel-Explainable-AI-Tools)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454a6685",
   "metadata": {},
   "source": [
    "## 1. Import dependencies and setup parameters\n",
    "\n",
    "This notebook assumes that you have already followed the instructions in the [README.md](/notebooks/README.md) to setup a PyTorch environment with all the dependencies required to run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2b3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import intel_extension_for_pytorch as ipex\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch\n",
    "import warnings\n",
    "import typing\n",
    "import pickle\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import ClassLabel, load_dataset, load_metric, Split\n",
    "from datasets import logging as datasets_logging\n",
    "from transformers.utils import logging as transformers_logging\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    get_scheduler\n",
    ")\n",
    "from tlt.utils.file_utils import download_and_extract_zip_file\n",
    "\n",
    "# Set the logging stream to stdout\n",
    "for handler in transformers_logging._get_library_root_logger().handlers:\n",
    "    handler.setStream(sys.stdout)\n",
    "\n",
    "sh = datasets_logging.logging.StreamHandler(sys.stdout)\n",
    "\n",
    "datasets_logging.set_verbosity_error()\n",
    "warnings.filterwarnings('ignore')\n",
    "os.environ[\"TRANSFORMERS_NO_ADVISORY_WARNINGS\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdf13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the name of the Hugging Face pretrained model to use (https://huggingface.co/models)\n",
    "# For example: \n",
    "#   albert-base-v2\n",
    "#   bert-base-uncased\n",
    "#   distilbert-base-uncased\n",
    "#   distilbert-base-uncased-finetuned-sst-2-english\n",
    "#   roberta-base\n",
    "model_name = \"distilbert-base-uncased\"\n",
    "\n",
    "# Define an output directory\n",
    "output_dir = os.environ[\"OUTPUT_DIR\"] if \"OUTPUT_DIR\" in os.environ else \\\n",
    "    os.path.join(os.environ[\"HOME\"], \"output\", model_name)\n",
    "\n",
    "# Define a dataset directory\n",
    "dataset_dir = os.environ[\"DATASET_DIR\"] if \"DATASET_DIR\" in os.environ else \\\n",
    "    os.path.join(os.environ[\"HOME\"], \"dataset\")\n",
    "\n",
    "print(\"Model name:\", model_name)\n",
    "print(\"Output directory:\", output_dir)\n",
    "print(\"Dataset directory:\", dataset_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f258d4f",
   "metadata": {},
   "source": [
    "## 2. Prepare the dataset\n",
    "\n",
    "The notebook has two options for getting a dataset:\n",
    "* Option A: Use a dataset from the [Hugging Face Datasets catalog](https://huggingface.co/datasets)\n",
    "* Option B: Use a custom dataset (downloaded from another source or from your local system)\n",
    "\n",
    "In both cases, the code ends up defining [`datasets.Dataset`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset) objects for the train and evaluation splits.\n",
    "\n",
    "Execute the following cell to load the tokenizer and declare the base class used for the dataset setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649c5c22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "class TextClassificationData():\n",
    "    \"\"\"\n",
    "    Base class used for defining the text classification dataset being used. Defines Hugging Face datasets.Dataset\n",
    "    objects for train and evaluations splits, along with helper functions for preprocessing the dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dataset_name, tokenizer, sentence1_key, sentence2_key, label_key):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.dataset_name = dataset_name\n",
    "        self.class_labels = None\n",
    "        \n",
    "        # Tokenized train and eval ds\n",
    "        self.train_ds = None\n",
    "        self.eval_ds = None\n",
    "        \n",
    "        # Column keys\n",
    "        self.sentence1_key = sentence1_key\n",
    "        self.sentence2_key = sentence2_key\n",
    "        self.label_key = label_key\n",
    "        \n",
    "    def tokenize_function(self, examples):\n",
    "        # Define the tokenizer args, depending on if the data has 2 sentences or just 1\n",
    "        args = ((examples[self.sentence1_key],) if self.sentence2_key is None \\\n",
    "                 else (examples[self.sentence1_key], examples[self.sentence2_key]))\n",
    "        return self.tokenizer(*args, padding=\"max_length\", truncation=True)\n",
    "    \n",
    "    def tokenize_dataset(self, dataset):\n",
    "        # Apply the tokenize function to the dataset\n",
    "        tokenized_dataset = dataset.map(self.tokenize_function, batched=True)\n",
    "\n",
    "        # Remove the raw text from the tokenized dataset\n",
    "        raw_text_columns = [self.sentence1_key, self.sentence2_key] if self.sentence2_key else [self.sentence1_key]\n",
    "        return tokenized_dataset.remove_columns(raw_text_columns)\n",
    "        \n",
    "    def define_train_eval_splits(self, dataset, train_split_name, eval_split_name, train_size=None, eval_size=None):\n",
    "        self.train_ds = dataset[train_split_name].shuffle().select(range(train_size)) if train_size \\\n",
    "            else tokenized_dataset[train_split_name]    \n",
    "        self.eval_ds = dataset[eval_split_name].shuffle().select(range(eval_size)) if eval_size \\\n",
    "            else tokenized_dataset[eval_split_name]\n",
    "        \n",
    "    def get_label_names(self):\n",
    "        if self.class_labels:\n",
    "            return self.class_labels.names\n",
    "        else:\n",
    "            raise ValueError(\"Class labels were not defined\")\n",
    "        \n",
    "    def display_sample(self, split_name=\"train\", sample_size=7):\n",
    "        # Display a sample of the raw data\n",
    "        sentence1_sample = self.dataset[split_name][self.sentence1_key][:sample_size]\n",
    "        sentence2_sample = self.dataset[split_name][self.sentence2_key][:sample_size] if self.sentence2_key else None\n",
    "        label_sample = self.dataset[split_name][self.label_key][:sample_size]\n",
    "        dataset_sample = zip(sentence1_sample, sentence2_sample, label_sample) if self.sentence2_key \\\n",
    "            else zip(sentence1_sample, label_sample)\n",
    "\n",
    "        columns = [self.sentence1_key, self.sentence2_key, self.label_key] if self.sentence2_key else \\\n",
    "            [self.sentence1_key, self.label_key]\n",
    "\n",
    "        # Display the sample using a dataframe\n",
    "        sample = pd.DataFrame(dataset_sample, columns=columns)\n",
    "        return sample.style.hide_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8512e1",
   "metadata": {},
   "source": [
    "Now that the base class is defined, either run [Option A to use the Hugging Face Dataset catalog](#Option-A:-Use-a-Hugging-Face-dataset) or [Option B for a custom dataset](#Option-B:-Use-a-custom-dataset) downloaded from online or from your local system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640e5611",
   "metadata": {},
   "source": [
    "### Option A: Use a Hugging Face dataset\n",
    "\n",
    "[Hugging Face Datasets](https://huggingface.co/datasets) has a catalog of datasets that can be specified by name. Information about the dataset is available in the catalog (including information on the size of the dataset and the splits).\n",
    "\n",
    "The next cell gets the [IMDb movie review dataset](https://huggingface.co/datasets/imdb) using the Hugging Face datasets API. If the notebook is executed multiple times, the dataset will be used from the dataset directory, to speed up the time that it takes to run.\n",
    "\n",
    "The IMDb dataset in Hugging Face has 3 splits: `train`, `test`, and `unsupervised`. This notebook will be using data from the `train` split for training and data from the `test` split for evaluation. The data has 2 columns: `text` (string with the movie review) and `label` (integer class label). The code in the next cell is setup to run using the IMDb dataset, so note that if a different dataset is being used, you may need to change the split names and/or the column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1d5fc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFDSTextClassificationData(TextClassificationData):\n",
    "    \"\"\"\n",
    "    Class used for loading and preprocessing text classification datasets from the Hugging Face datasets catalog\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, dataset_dir, dataset_name, train_size, eval_size, train_split_name,\n",
    "                 eval_split_name, sentence1_key, sentence2_key, label_key):\n",
    "        \"\"\"\n",
    "        Initialize the HFDSTextClassificationData class for a text classification dataset from Hugging Face.\n",
    "        \n",
    "        :param tokenizer: Tokenizer to preprocess the dataset\n",
    "        :param dataset_dir: Cache directory used when loading the dataset\n",
    "        :param dataset_name: Name of the dataset to load from the Hugging Face catalog\n",
    "        :param train_size: Size of the training dataset. For quicker training or debug, use a subset of the data.\n",
    "                           Set to `None` to use all the data.\n",
    "        :param eval_size: Size of the evaluation dataset.\n",
    "        :param train_split_name: String specifying which split to load for training (e.g. \"train[:80%]\"). See the\n",
    "                                 https://www.tensorflow.org/datasets/splits documentation for more information on\n",
    "                                 defining splits.\n",
    "        :param eval_split_name: String specifying the split to load for evaluation.\n",
    "        :param sentence1_key: Name of the sentence1 column\n",
    "        :param sentence2_key: Name of the sentence2 column or `None` if there's only one text column\n",
    "        :param label_key: Name of the label column\n",
    "        \"\"\"\n",
    "\n",
    "        # Init base class\n",
    "        TextClassificationData.__init__(self, dataset_name, tokenizer, sentence1_key, sentence2_key, label_key) \n",
    "        \n",
    "        # Load the dataset from the Hugging Face dataset API\n",
    "        self.dataset = load_dataset(dataset_name, cache_dir=dataset_dir)\n",
    "\n",
    "        # Tokenize the dataset\n",
    "        tokenized_dataset = self.tokenize_dataset(self.dataset)\n",
    "\n",
    "        # Get the training and eval dataset based on the specified dataset sizes\n",
    "        self.define_train_eval_splits(tokenized_dataset, train_split_name, eval_split_name, train_size, eval_size)\n",
    "\n",
    "        # Save the class label information to use later when predicting\n",
    "        self.class_labels = self.dataset[train_split_name].features[label_key]\n",
    "\n",
    "# Name of the Hugging Face dataset\n",
    "dataset_name = \"imdb\"\n",
    "\n",
    "# For quicker training and debug runs, use a subset of the dataset by specifying the size of the train/eval datasets.\n",
    "# Set the sizes `None` to use the full dataset. The full IMDb dataset has 25,000 training and 25,000 test examples.\n",
    "train_dataset_size = 1000\n",
    "eval_dataset_size = 1000\n",
    "\n",
    "# Name of the columns in the dataset (the column names may vary if you are not using the IMDb dataset)\n",
    "sentence1_key = \"text\"\n",
    "sentence2_key = None\n",
    "label_key = \"label\"\n",
    "\n",
    "dataset = HFDSTextClassificationData(tokenizer, dataset_dir, dataset_name, train_dataset_size, eval_dataset_size,\n",
    "                                     Split.TRAIN, Split.TEST, sentence1_key, sentence2_key, label_key)\n",
    "\n",
    "# Print a sample of the data\n",
    "dataset.display_sample(Split.TRAIN, sample_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362625ec",
   "metadata": {},
   "source": [
    "Skip to Step 3 [Get the model and setup the Trainer](#3.-Get-the-model-and-setup-the-Trainer) to continue using the dataset from the Hugging Face catalog."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c0ba36",
   "metadata": {},
   "source": [
    "### Option B: Use a custom dataset\n",
    "\n",
    "Instead of using a dataset from the Hugging Face dataset catalog, a custom dataset from your local system or a download can be used.\n",
    "\n",
    "In this example, we download the [SMS Spam Collection dataset](https://archive-beta.ics.uci.edu/ml/datasets/sms+spam+collection). The zip file has a single tab-separated value file with two columns. The first column is the label (`ham` or `spam`) and the second column is the text of the SMS message:\n",
    "```\n",
    "<ham or spam>\t<text>\n",
    "<ham or spam>\t<text>\n",
    "<ham or spam>\t<text>\n",
    "...\n",
    "```\n",
    "If you are using a custom dataset that has a similarly formatted csv or tsv file, you can use the class defined below. Create your object by passing in custom values for csv file name, delimiter, the label map, mapping function, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c706bfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomCsvTextClassificationData(TextClassificationData):\n",
    "    \"\"\"\n",
    "    Class used for loading and preprocessing text classification datasets from CSV files\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, dataset_name, dataset_dir, data_files, delimiter, label_names, sentence1_key, sentence2_key,\n",
    "                 label_key, train_percent=0.8, eval_percent=0.2, train_size=None, eval_size=None, map_function=None):\n",
    "        \"\"\"\n",
    "        Intialize the CustomCsvTextClassificationData class for a text classification\n",
    "        dataset. The classes uses the Hugging Face datasets API to load the CSV file,\n",
    "        and split it into a train and eval datasets based on the specified percentages.\n",
    "        If train_size and eval_size are also defined, the datasets are reduced to the\n",
    "        specified number of examples.\n",
    "        \n",
    "        :param tokenizer: Tokenizer to preprocess the dataset\n",
    "        :param dataset_name: Dataset name for identification purposes\n",
    "        :param dataset_dir: Directory where the csv file(s) are located\n",
    "        :param data_files: List of data file names\n",
    "        :param delimiter: Delimited for the csv files\n",
    "        :param label_names: List of label names\n",
    "        :param sentence1_key: Name of the sentence1 column\n",
    "        :param sentence2_key: Name of the sentence2 column or `None` if there's only one text column\n",
    "        :param label_key: Name of the label column\n",
    "        :param train_percent: Decimal value for the percentage of the dataset that should be used for training\n",
    "                              (e.g. 0.8 for 80%)\n",
    "        :param eval_percent: Decimal value for the percentage of the dataset that should used for validation\n",
    "                             (e.g. 0.2 for 20%)\n",
    "        :param train_size: Size of the training dataset. For quicker training or debug, use a subset of the data.\n",
    "                           Set to `None` to use all the data.\n",
    "        :param eval_size: Size of the eval dataset. Set to `None` to use all the data.\n",
    "        :param map_function: (Optional) Map function to apply to the dataset. For example, if the csv file has string\n",
    "                             labels instead of numerical values, map function can do the conversion.\n",
    "        \"\"\"\n",
    "        # Init base class\n",
    "        TextClassificationData.__init__(self, dataset_name, tokenizer, sentence1_key, sentence2_key, label_key)\n",
    "        \n",
    "        if (train_percent + eval_percent) > 1:\n",
    "            raise ValueError(\"The combined value of the train percentage and eval percentage \" \\\n",
    "                             \"cannot be greater than 1\")\n",
    "        \n",
    "        # Create a list of the column names\n",
    "        column_names = [label_key, sentence1_key, sentence2_key] if sentence2_key else [label_key, sentence1_key]\n",
    "        \n",
    "        # Load the dataset using the Hugging Face API\n",
    "        self.dataset = load_dataset(dataset_dir, delimiter=delimiter, data_files=data_files, column_names=column_names)\n",
    "        \n",
    "        # Optionally map the dataset labels using the map_function\n",
    "        if map_function:\n",
    "            self.dataset = self.dataset.map(map_function)\n",
    "        \n",
    "        # Setup the class labels\n",
    "        self.class_labels = ClassLabel(num_classes=len(label_names), names=label_names)\n",
    "        self.dataset[Split.TRAIN].features[label_key] = self.class_labels\n",
    "        \n",
    "        # Split the dataset based on the percentages defined\n",
    "        self.dataset = self.dataset[Split.TRAIN].train_test_split(train_size=train_percent, test_size=eval_percent)\n",
    "        \n",
    "        # Tokenize the dataset\n",
    "        tokenized_dataset = self.tokenize_dataset(self.dataset)\n",
    "\n",
    "        # Get the training and eval dataset based on the specified dataset sizes\n",
    "        self.define_train_eval_splits(tokenized_dataset, Split.TRAIN, Split.TEST, train_size, eval_size)\n",
    "\n",
    "\n",
    "# Modify the variables below to use a different dataset or a csv file on your local system.\n",
    "# The csv_path variable should be pointing to a csv file with 2 columns (the label and the text)\n",
    "dataset_url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00228/smsspamcollection.zip\"\n",
    "dataset_dir = os.path.join(dataset_dir, \"smsspamcollection\")\n",
    "csv_name = \"SMSSpamCollection\"\n",
    "delimiter = \"\\t\"\n",
    "label_names = [\"ham\", \"spam\"]\n",
    "\n",
    "# Rename the file to include the csv extension so that the dataset API knows how to load the file\n",
    "renamed_csv = \"{}.csv\".format(csv_name)\n",
    "\n",
    "# If we don't already have the csv file, download and extract the zip file to get it.\n",
    "if not os.path.exists(os.path.join(dataset_dir, csv_name)) and \\\n",
    "                      not os.path.exists(os.path.join(dataset_dir, renamed_csv)):\n",
    "    download_and_extract_zip_file(dataset_url, dataset_dir)\n",
    "\n",
    "if not os.path.exists(os.path.join(dataset_dir, renamed_csv)):\n",
    "    os.rename(os.path.join(dataset_dir, csv_name), os.path.join(dataset_dir, renamed_csv))\n",
    "    \n",
    "# Columns\n",
    "sentence1_key = \"text\"\n",
    "sentence2_key = None\n",
    "label_key = \"label\"\n",
    "\n",
    "# Map function to translate labels in the csv file to numerical values when loading the dataset\n",
    "def map_spam(example):\n",
    "    example[\"label\"] = int(example[\"label\"] == \"spam\")\n",
    "    return example\n",
    "\n",
    "dataset = CustomCsvTextClassificationData(tokenizer, \"smsspamcollection\", dataset_dir, [renamed_csv], delimiter,\n",
    "                                          label_names, sentence1_key, sentence2_key, label_key, train_size=1000,\n",
    "                                          eval_size=1000, map_function=map_spam)\n",
    "\n",
    "# Print a sample of the data\n",
    "dataset.display_sample(Split.TRAIN, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a24bcd",
   "metadata": {},
   "source": [
    "## 3. Prepare the Model for Fine Tuning and Evaluation\n",
    "\n",
    "The notebook has two options to train the model.\n",
    "\n",
    "- Option A: Use the [`Trainer`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.Trainer) API from Hugging Face.\n",
    "- Option B: Use the native PyTorch API.\n",
    "\n",
    "In both cases, the model ends up being a transformers model and depending on the class constructor arguments, the appropriate API is selected.\n",
    "\n",
    "Execute the following cell to declare the base class used for the Text Classification Model setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797485aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationModel():\n",
    "    \"\"\"\n",
    "    Class used for model loading, training and evaluation.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 model_name: str, \n",
    "                 num_labels: int, \n",
    "                 training_args: TrainingArguments = None, \n",
    "                 ipex_optimize: bool = True, \n",
    "                 device: str = \"cpu\"):\n",
    "        \"\"\"\n",
    "        Initialize the TextClassificationModel class for a text classification model with\n",
    "        PyTorch. The class uses the model_name to load the pre-trained PyTorch model from\n",
    "        Hugging Face. If the training_args are given then the Trainer API is selected for\n",
    "        training and evaluation of the model otherwise native PyTorch API is selected for\n",
    "        model training and evaluation\n",
    "        \n",
    "        :param model_name: Name of the pre-trained model to load from Hugging Face\n",
    "        :param num_labels: Number of class labels\n",
    "        :param training_args: A TrainingArguments object if using the Trainer API to train\n",
    "                              the model. If None, native PyTorch API is used for training.\n",
    "        :param ipex_optimize: If True, then the model is optimized to run on intel hardware.\n",
    "        :param device: Device to run on the PyTorch model.\n",
    "        \"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.num_labels = num_labels\n",
    "        self.training_args = training_args\n",
    "        self.device = device\n",
    "        self.trainer = None\n",
    "        \n",
    "        self.train_ds = dataset.train_ds\n",
    "        self.eval_ds = dataset.eval_ds\n",
    "        \n",
    "        # Load the model using the pretrained weights\n",
    "        self.model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n",
    "       \n",
    "        # Apply the ipex optimize function to the model\n",
    "        if ipex_optimize:\n",
    "            self.model = ipex.optimize(self.model)\n",
    "            \n",
    "    def train(self, \n",
    "              dataset: TextClassificationData,\n",
    "              optimizers: typing.Tuple[torch.optim.Optimizer, torch.optim.lr_scheduler.LambdaLR],\n",
    "              num_train_epochs: int = 1,\n",
    "              batch_size: int = 16,\n",
    "              compute_metrics: typing.Callable = None,\n",
    "              shuffle_samples: bool = True\n",
    "             ):\n",
    "\n",
    "        # If training_args are given, we use the `Trainer` API to train the model\n",
    "        if self.training_args:\n",
    "            self.model.train()\n",
    "            self.trainer = Trainer(model=self.model,\n",
    "                                   args=self.training_args,\n",
    "                                   train_dataset=self.train_ds,\n",
    "                                   eval_dataset=self.eval_ds,\n",
    "                                   optimizers=optimizers,\n",
    "                                   compute_metrics=compute_metrics)\n",
    "            self.trainer.train()\n",
    "            \n",
    "        # If training_args are not given, we use native PyTorch API to train the model\n",
    "        else:\n",
    "            \n",
    "            # Rename the `label` column to `labels` because the model expects the argument to be named `labels`\n",
    "            self.train_ds = self.train_ds.rename_column(\"label\", \"labels\")\n",
    "            \n",
    "            # Set the format of the dataset to return PyTorch tensors instead of lists\n",
    "            self.train_ds.set_format(\"torch\")\n",
    "            \n",
    "            train_dataloader = DataLoader(self.train_ds, shuffle=shuffle_samples, batch_size=batch_size)\n",
    "            \n",
    "            # Unpack the `optimizers` parameter to get optimizer and lr_scheduler\n",
    "            optimizer, lr_scheduler = optimizers[0], optimizers[1]\n",
    "            \n",
    "            # Define number of training steps for the training progress bar\n",
    "            num_training_steps = num_train_epochs * len(train_dataloader)\n",
    "            progress_bar = tqdm(range(num_training_steps))\n",
    "            \n",
    "            # Training loop\n",
    "            self.model.to(self.device)\n",
    "            self.model.train()\n",
    "            for epoch in range(num_train_epochs):\n",
    "                for batch in train_dataloader:\n",
    "                    batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                    outputs = self.model(**batch)\n",
    "                    loss = outputs.loss\n",
    "                    loss.backward()\n",
    "\n",
    "                    optimizer.step()\n",
    "                    lr_scheduler.step()\n",
    "                    optimizer.zero_grad()\n",
    "                    progress_bar.update(1)\n",
    "    \n",
    "    def evaluate(self, batch_size=16):\n",
    "        \n",
    "        if self.trainer:\n",
    "            self.model.eval()\n",
    "            metrics = self.trainer.evaluate()\n",
    "            for key in metrics.keys():\n",
    "                print(\"{}: {}\".format(key, metrics[key]))\n",
    "        else:\n",
    "            # Rename the `label` column to `labels` because the model expects the argument to be named `labels`\n",
    "            self.eval_ds = self.eval_ds.rename_column(\"label\", \"labels\")\n",
    "            \n",
    "            # Set the format of the dataset to return PyTorch tensors instead of lists\n",
    "            self.eval_ds.set_format(\"torch\")\n",
    "            \n",
    "            eval_dataloader = DataLoader(self.eval_ds, batch_size=batch_size)\n",
    "            progress_bar = tqdm(range(len(eval_dataloader)))\n",
    "            \n",
    "            metric = load_metric(\"accuracy\")\n",
    "            self.model.eval()\n",
    "            for batch in eval_dataloader:\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                with torch.no_grad():\n",
    "                    outputs = self.model(**batch)\n",
    "\n",
    "                logits = outputs.logits\n",
    "                predictions = torch.argmax(logits, dim=-1)\n",
    "                metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n",
    "                progress_bar.update(1)\n",
    "\n",
    "            print(metric.compute())\n",
    "            \n",
    "    def predict(self, raw_input_text):\n",
    "        if isinstance(raw_input_text, str):\n",
    "            raw_input_text = [raw_input_text]\n",
    "        \n",
    "        # Encode the raw text using the tokenizer\n",
    "        encoded_input = tokenizer(raw_input_text, padding=True, return_tensors='pt')\n",
    "        \n",
    "        # Input the encoded text(s) to the model and get the predicted results\n",
    "        output = self.model(**encoded_input)\n",
    "        _, predictions = torch.max(output.logits, dim=1)\n",
    "        \n",
    "        # Translate the predictions to class label strings\n",
    "        prediction_labels = dataset.class_labels.int2str(predictions)\n",
    "\n",
    "        # Create a dataframe to display the results\n",
    "        result_list = [list(x) for x in zip(raw_text_input, prediction_labels)]\n",
    "        result_df = pd.DataFrame(result_list, columns=[\"Input Text\", \"Predicted Label\"])\n",
    "        return result_df.style.hide_index()\n",
    "    \n",
    "    def parameters(self):\n",
    "        return self.model.parameters()\n",
    "    \n",
    "    def save(self, output_dir):\n",
    "        self.model.save_pretrained(output_dir)\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, output_dir):\n",
    "        return cls(output_dir, num_labels=len(dataset.get_label_names()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e8f1edd",
   "metadata": {},
   "source": [
    "Now that the `TextClassificationModel` class is defined, either use Option A to use the [`Trainer`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.Trainer) API from Hugging Face or Option B to use the native PyTorch API."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a606f16",
   "metadata": {},
   "source": [
    "### Option A: Use the [`Trainer`](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.Trainer) API from Hugging Face\n",
    "\n",
    "This step gets the pretrained model from [Hugging Face](https://huggingface.co/models) and sets up the\n",
    "[TrainingArguments](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.TrainingArguments) and the\n",
    "[Trainer](https://huggingface.co/docs/transformers/v4.16.2/en/main_classes/trainer#transformers.Trainer). For simplicity, this example is using default values for most of the training args, but we are specifying our output directory and the number of training epochs. If your output directory already has checkpoints from a previous run,\n",
    "training will resume from the last checkpoint. The `overwrite_output_dir` training argument can be set to\n",
    "`True` if you want to instead overwrite previously generated checkpoints.\n",
    "\n",
    "> Note that it is expected to see a warning at this step about some weights not being used. This is because\n",
    "> the pretraining head from the original model is being replaced with a classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d70a4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 2\n",
    "batch_size = 16\n",
    "num_labels = len(dataset.get_label_names())\n",
    "\n",
    "# Define a TrainingArguments object for the Trainer API to use.\n",
    "training_args = TrainingArguments(output_dir=output_dir, num_train_epochs=num_train_epochs)\n",
    "\n",
    "# Get the model from Hugging Face. Since we are specifying training_args, the model is trained and\n",
    "# evaluated with the Trainer API.\n",
    "model = TextClassificationModel(model_name=model_name, num_labels=num_labels, training_args=training_args)\n",
    "\n",
    "# Define model training parameters\n",
    "learning_rate      = 5e-5\n",
    "optimizer          = AdamW(model.parameters(), lr=learning_rate)\n",
    "num_training_steps = num_train_epochs * len(dataset.train_ds)\n",
    "metric             = load_metric(\"accuracy\")\n",
    "lr_scheduler       = get_scheduler(\n",
    "                        name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "                     )\n",
    "\n",
    "# Helper function for the Trainer API to compute metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fcabd97",
   "metadata": {},
   "source": [
    "**Train and evaluate the model with the Trainer API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1256c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(\n",
    "    dataset, \n",
    "    optimizers=(optimizer, lr_scheduler), \n",
    "    num_train_epochs=num_train_epochs, \n",
    "    batch_size=batch_size,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a7afe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce10679",
   "metadata": {},
   "source": [
    "### Option B: Use the native PyTorch API\n",
    "\n",
    "This step gets the pretrained model from [Hugging Face](https://huggingface.co/models) and uses native PyTorch API to train and evaluate the model.\n",
    "\n",
    "> Note that it is expected to see a warning at this step about some weights not being used. This is because\n",
    "> the pretraining head from the original model is being replaced with a classification head."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202f6b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_train_epochs = 2\n",
    "batch_size = 16\n",
    "num_labels = len(dataset.get_label_names())\n",
    "\n",
    "# Get the model from Hugging Face. Since we are not specifying training_args, the model is trained and\n",
    "# evaluated with the native PyTorch API.\n",
    "model = TextClassificationModel(model_name=model_name, num_labels=num_labels)\n",
    "\n",
    "# Define model training parameters\n",
    "learning_rate      = 5e-5\n",
    "optimizer          = AdamW(model.parameters(), lr=learning_rate)\n",
    "num_training_steps = num_train_epochs * len(dataset.train_ds)\n",
    "lr_scheduler       = get_scheduler(\n",
    "                        name=\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4821e46",
   "metadata": {},
   "source": [
    "**Train and evaluate the model with the native PyTorch API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb2e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(\n",
    "    dataset, \n",
    "    optimizers=(optimizer, lr_scheduler), \n",
    "    num_train_epochs=num_train_epochs, \n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff3b884",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83b873f",
   "metadata": {},
   "source": [
    "## 4. Export the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faa4beb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to our output directory\n",
    "model.save(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49449342",
   "metadata": {},
   "source": [
    "## 5. Reload the model and make predictions\n",
    "\n",
    "The output directory is used to reload the model. In the next cell, we evalute the reloaded model to verify that we are getting the same metrics that we saw after fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e339d50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "reloaded_model = TextClassificationModel.load(output_dir)\n",
    "    \n",
    "reloaded_model.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70a5386",
   "metadata": {},
   "source": [
    "Next, we demonstrate how encode raw text input and get predictions from the reloaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f084d0ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = reloaded_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d231c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup some raw text input\n",
    "raw_text_input = [\"It was okay. I finished it, but wouldn't watch it again.\",\n",
    "                  \"So bad\",\n",
    "                  \"Definitely not my favorite\",\n",
    "                  \"Highly recommended\"]\n",
    "\n",
    "model.predict(raw_text_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bdc89b",
   "metadata": {},
   "source": [
    "## 6. Get Explainations with Intel Explainable AI Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7144cfca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from intel_ai_safety.explainer import attributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34224ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import softmax\n",
    "# Define a prediction function\n",
    "def f(x):\n",
    "    encoded_input = tokenizer(x.tolist(), padding='max_length', max_length=512, truncation=True, return_tensors='pt')\n",
    "    outputs = model.model(**encoded_input)\n",
    "    return softmax(outputs.logits.detach().numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfe9faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from intel_ai_safety.explainer import attributions\n",
    "# Get shap values\n",
    "text_for_shap = dataset.dataset['test'][:10]['text']\n",
    "partition_explainer = attributions.partition_text_explainer(f, dataset.class_labels.names, text_for_shap, r\"\\W+\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1e308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_explainer.visualize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bee40324",
   "metadata": {},
   "source": [
    "## Citations\n",
    "\n",
    "```\n",
    "@InProceedings{maas-EtAl:2011:ACL-HLT2011,\n",
    "  author    = {Maas, Andrew L.  and  Daly, Raymond E.  and  Pham, Peter T.  and  Huang, Dan  and  Ng, Andrew Y.  and  Potts, Christopher},\n",
    "  title     = {Learning Word Vectors for Sentiment Analysis},\n",
    "  booktitle = {Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies},\n",
    "  month     = {June},\n",
    "  year      = {2011},\n",
    "  address   = {Portland, Oregon, USA},\n",
    "  publisher = {Association for Computational Linguistics},\n",
    "  pages     = {142--150},\n",
    "  url       = {http://www.aclweb.org/anthology/P11-1015}\n",
    "}\n",
    "\n",
    "@misc{misc_sms_spam_collection_228,\n",
    "  author       = {Almeida, Tiago},\n",
    "  title        = {{SMS Spam Collection}},\n",
    "  year         = {2012},\n",
    "  howpublished = {UCI Machine Learning Repository}\n",
    "}\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
