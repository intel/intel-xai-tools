{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e2893de",
   "metadata": {},
   "source": [
    "# Creating Model Card for Toxic Comments Classification in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb56f9e1",
   "metadata": {},
   "source": [
    "Adapted form [Tensorflow](https://colab.research.google.com/github/google/eng-edu/blob/main/ml/pc/exercises/fairness_text_toxicity_part1.ipynb?utm_source=practicum-fairness&utm_campaign=colab-external&utm_medium=referral&utm_content=fairnessexercise1-colab#scrollTo=2z_xzJ40j9Q-) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64c2921a",
   "metadata": {},
   "source": [
    "#### Training Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13462d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tempfile\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow as tf\n",
    "import tensorflow_model_analysis as tfma\n",
    "import tensorflow_data_validation as tfdv\n",
    "\n",
    "from tensorflow_model_analysis.addons.fairness.post_export_metrics import fairness_indicators\n",
    "from tensorflow_model_analysis.addons.fairness.view import widget_view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cd35c4",
   "metadata": {},
   "source": [
    "#### Model Card Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1d8721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from intel_ai_safety.model_card_gen.model_card_gen import ModelCardGen\n",
    "from intel_ai_safety.model_card_gen.datasets import TensorflowDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23dba98",
   "metadata": {},
   "source": [
    "## Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4ccc39",
   "metadata": {},
   "source": [
    "#### Data Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec83155f",
   "metadata": {},
   "source": [
    "This version of the CivilComments Dataset provides access to the primary seven labels that were annotated by crowd workers, the toxicity and other tags are a value between 0 and 1 indicating the fraction of annotators that assigned these attributes to the comment text.\n",
    "\n",
    "The other tags are only available for a fraction of the input examples. They are currently ignored for the main dataset; the CivilCommentsIdentities set includes those labels, but only consists of the subset of the data with them. The other attributes that were part of the original CivilComments release are included only in the raw data. See the Kaggle documentation for more details about the available features.\n",
    "\n",
    "The comments in this dataset come from an archive of the Civil Comments platform, a commenting plugin for independent news sites. These public comments were created from 2015 - 2017 and appeared on approximately 50 English-language news sites across the world. When Civil Comments shut down in 2017, they chose to make the public comments available in a lasting open archive to enable future research. The original data, published on figshare, includes the public comment text, some associated metadata such as article IDs, timestamps and commenter-generated \"civility\" labels, but does not include user ids. Jigsaw extended this dataset by adding additional labels for toxicity, identity mentions, as well as covert offensiveness. This data set is an exact replica of the data released for the Jigsaw Unintended Bias in Toxicity Classification Kaggle challenge. This dataset is released under CC0, as is the underlying comment text.\n",
    "\n",
    "For comments that have a parent_id also in the civil comments data, the text of the previous comment is provided as the \"parent_text\" feature. Note that the splits were made without regard to this information, so using previous comments may leak some information. The annotators did not have access to the parent text when making the labels.\n",
    "\n",
    "*source*: https://www.tensorflow.org/datasets/catalog/civil_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e72a0fb",
   "metadata": {},
   "source": [
    "```\n",
    "@misc{pavlopoulos2020toxicity,\n",
    "    title={Toxicity Detection: Does Context Really Matter?},\n",
    "    author={John Pavlopoulos and Jeffrey Sorensen and Lucas Dixon and Nithum Thain and Ion Androutsopoulos},\n",
    "    year={2020}, eprint={2006.00998}, archivePrefix={arXiv}, primaryClass={cs.CL}\n",
    "}\n",
    "\n",
    "@article{DBLP:journals/corr/abs-1903-04561,\n",
    "  author    = {Daniel Borkan and\n",
    "               Lucas Dixon and\n",
    "               Jeffrey Sorensen and\n",
    "               Nithum Thain and\n",
    "               Lucy Vasserman},\n",
    "  title     = {Nuanced Metrics for Measuring Unintended Bias with Real Data for Text\n",
    "               Classification},\n",
    "  journal   = {CoRR},\n",
    "  volume    = {abs/1903.04561},\n",
    "  year      = {2019},\n",
    "  url       = {http://arxiv.org/abs/1903.04561},\n",
    "  archivePrefix = {arXiv},\n",
    "  eprint    = {1903.04561},\n",
    "  timestamp = {Sun, 31 Mar 2019 19:01:24 +0200},\n",
    "  biburl    = {https://dblp.org/rec/bib/journals/corr/abs-1903-04561},\n",
    "  bibsource = {dblp computer science bibliography, https://dblp.org}\n",
    "}\n",
    "\n",
    "@inproceedings{pavlopoulos-etal-2021-semeval,\n",
    "    title = \"{S}em{E}val-2021 Task 5: Toxic Spans Detection\",\n",
    "    author = \"Pavlopoulos, John  and Sorensen, Jeffrey  and Laugier, L{'e}o and Androutsopoulos, Ion\",\n",
    "    booktitle = \"Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021)\",\n",
    "    month = aug,\n",
    "    year = \"2021\",\n",
    "    address = \"Online\",\n",
    "    publisher = \"Association for Computational Linguistics\",\n",
    "    url = \"https://aclanthology.org/2021.semeval-1.6\",\n",
    "    doi = \"10.18653/v1/2021.semeval-1.6\",\n",
    "    pages = \"59--69\",\n",
    "}\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2a482dc",
   "metadata": {},
   "source": [
    "**Feature documentation**:\n",
    "\n",
    "|Feature|Class|Dtype|\n",
    "|-------|:---:|:---:|\n",
    "|article_id|\tTensor|\t\ttf.int32|\n",
    "|id|\tTensor|\t\ttf.string|\n",
    "|identity_attack|\tTensor|\t\ttf.float32|\n",
    "|insult|\tTensor|\t\ttf.float32|\n",
    "|obscene|\tTensor|\t\ttf.float32|\n",
    "|parent_id|\tTensor|\t\ttf.int32|\n",
    "|parent_text|\tText|\t\ttf.string|\n",
    "|severe_toxicity|\tTensor|\t\ttf.float32|\n",
    "|sexual_explicit|\tTensor|\t\ttf.float32|\n",
    "|text|\tText|\t\ttf.string|\n",
    "|threat|\tTensor|\t\ttf.float32|\n",
    "|toxicity|\tTensor|\t\ttf.float32|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a79713ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_url = 'https://storage.googleapis.com/civil_comments_dataset/'\n",
    "\n",
    "train_tf_file = tf.keras.utils.get_file('train_tf_processed.tfrecord',\n",
    "                                        dataset_url + 'train_tf_processed.tfrecord')\n",
    "\n",
    "validate_tf_file = tf.keras.utils.get_file('validate_tf_processed.tfrecord',\n",
    "                                           dataset_url + 'validate_tf_processed.tfrecord')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c66decc",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fced58",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT_FEATURE = 'comment_text'\n",
    "LABEL = 'toxicity'\n",
    "\n",
    "FEATURE_MAP = {\n",
    "    LABEL: tf.io.FixedLenFeature([], tf.float32),\n",
    "    TEXT_FEATURE: tf.io.FixedLenFeature([], tf.string),\n",
    "    \n",
    "    'sexual_orientation': tf.io.VarLenFeature(tf.string),\n",
    "    'gender': tf.io.VarLenFeature(tf.string),\n",
    "    'religion': tf.io.VarLenFeature(tf.string),\n",
    "    'race': tf.io.VarLenFeature(tf.string),\n",
    "    'disability': tf.io.VarLenFeature(tf.string)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49471282",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_input_fn():\n",
    "    def parse_function(serialized):\n",
    "        # parse_single_example works on tf.train.Example type\n",
    "        parsed_example = tf.io.parse_single_example(serialized=serialized, features=FEATURE_MAP)\n",
    "        # fighting the 92%-8% imbalance in the dataset\n",
    "        # adding `weight` label, doesn't exist already (only FEATURE_MAP keys exist)\n",
    "        parsed_example['weight'] = tf.add(parsed_example[LABEL], 0.1)  # 0.1 for non-toxic, 1.1 for toxic\n",
    "        return (parsed_example, parsed_example[LABEL])  # (x, y)\n",
    "    \n",
    "\n",
    "    train_dataset = tf.data.TFRecordDataset(filenames=[train_tf_file]).map(parse_function).batch(512)\n",
    "    return train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aabaf8c",
   "metadata": {},
   "source": [
    "#### Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5f12c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorizing through TFHub\n",
    "embedded_text_feature_column = hub.text_embedding_column(\n",
    "    key=TEXT_FEATURE,\n",
    "    module_spec='https://tfhub.dev/google/nnlm-en-dim128/1')\n",
    "\n",
    "classifier = tf.estimator.DNNClassifier(\n",
    "    hidden_units=[500, 100],\n",
    "    weight_column='weight',\n",
    "    feature_columns=[embedded_text_feature_column],\n",
    "    optimizer=tf.keras.optimizers.legacy.Adagrad(learning_rate=0.003),\n",
    "    loss_reduction=tf.losses.Reduction.SUM,\n",
    "    n_classes=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e45a4a",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0352b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.train(input_fn=train_input_fn, steps=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f47eaa",
   "metadata": {},
   "source": [
    "## Export in EvalSavedModel Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e9e645",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_PATH = tempfile.gettempdir()\n",
    "\n",
    "def eval_input_receiver_fn():\n",
    "    serialized_tf_example = tf.compat.v1.placeholder(dtype=tf.string, shape=[None], name='input_example_placeholder')\n",
    "    \n",
    "    receiver_tensors = {'examples': serialized_tf_example}\n",
    "    features = tf.io.parse_example(serialized_tf_example, FEATURE_MAP)\n",
    "    features['weight'] = tf.ones_like(features[LABEL])\n",
    "    \n",
    "    return tfma.export.EvalInputReceiver(\n",
    "        features=features,\n",
    "        receiver_tensors=receiver_tensors,\n",
    "        labels=features[LABEL]\n",
    "    )\n",
    "\n",
    "tfma_export_dir = tfma.export.export_eval_savedmodel(\n",
    "    estimator = classifier,  # trained model\n",
    "    export_dir_base = MODEL_PATH,\n",
    "    eval_input_receiver_fn = eval_input_receiver_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4917e184",
   "metadata": {},
   "outputs": [],
   "source": [
    "# export EvalSavedModel \n",
    "tfma_export_dir = tfma.export.export_eval_savedmodel(\n",
    "    estimator = classifier,  # trained model\n",
    "    export_dir_base = MODEL_PATH,\n",
    "    eval_input_receiver_fn = eval_input_receiver_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c603c7be",
   "metadata": {},
   "source": [
    "## Making a Model Card"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc1445c",
   "metadata": {},
   "outputs": [],
   "source": [
    "_model_path = tfma_export_dir\n",
    "_data_paths = {'eval': TensorflowDataset(validate_tf_file),\n",
    "               'train': TensorflowDataset(train_tf_file)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766ebda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "_eval_config =  'eval_config.proto'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "630aff50",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_eval_config}\n",
    "\n",
    "model_specs {\n",
    "# To use EvalSavedModel set `signature_name` to \"eval\".\n",
    "signature_name: \"eval\"\n",
    "}\n",
    "\n",
    "## Post training metric information. These will be merged with any built-in\n",
    "## metrics from training.\n",
    "metrics_specs {\n",
    "metrics { class_name: \"BinaryAccuracy\" }\n",
    "metrics { class_name: \"Precision\" }\n",
    "metrics { class_name: \"Recall\" }\n",
    "metrics { class_name: \"ConfusionMatrixPlot\" }\n",
    "metrics { class_name: \"FairnessIndicators\" }\n",
    "}\n",
    "\n",
    "## Slicing information\n",
    "slicing_specs {}  # overall slice\n",
    "slicing_specs {\n",
    "feature_keys: [\"gender\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5d057d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mc = {\n",
    "  \"model_details\": {\n",
    "    \"name\": \"Detecting Toxic Comments\",\n",
    "    \"overview\":  (\n",
    "    'The Conversation AI team, a research initiative founded by Jigsaw and Google '\n",
    "    '(both part of Alphabet), builds technology to protect voices in conversation. '\n",
    "    'A main area of focus is machine learning models that can identify toxicity in '\n",
    "    'online conversations, where toxicity is defined as anything *rude, disrespectful '\n",
    "    'or otherwise likely to make someone leave a discussion*. '\n",
    "    'This multi-headed model attemps to recognize toxicity and several subtypes of toxicity: '\n",
    "    'This model recognizes toxicity and minimizes this type of unintended bias '\n",
    "    'with respect to mentions of identities. Reduce unintended bias ensured we can detect toxicity '\n",
    "    ' accross a wide range of conversations. '),\n",
    "    \"owners\": [\n",
    "      {\n",
    "        \"name\": \"Intel XAI Team\",\n",
    "        \"contact\": \"xai@intel.com\"\n",
    "      }\n",
    "    ],\n",
    "\n",
    "    \"references\": [\n",
    "      {\n",
    "        \"reference\": \"https://www.kaggle.com/c/jigsaw-unintended-bias-in-toxicity-classification/data\"\n",
    "      },\n",
    "      {\n",
    "        \"reference\": \"https://medium.com/jigsaw/unintended-bias-and-names-of-frequently-targeted-groups-8e0b81f80a23\"\n",
    "      }\n",
    "    ],\n",
    "    \"graphics\": {\n",
    "      \"description\": \" \"\n",
    "    }\n",
    "  },\n",
    "  \"considerations\": { \n",
    "      \"limitations\": [\n",
    "            {\"description\": ('Overrepresented Identities in Data:\\n'\n",
    "                    'Identity terms for more frequently targeted groups '\n",
    "                   '(e.g. words like “black”, “muslim”, “feminist”, “woman”, “gay” etc)'\n",
    "                   ' often have higher scores because comments about those groups are '\n",
    "                   'over-represented in abusive and toxic comments.')\n",
    "            },\n",
    "           {\"description\": ('False Positive Rate:\\n'\n",
    "                    'The names of targeted groups appear far more often in abusive '\n",
    "                    'comments. For example, in many forums unfortunately it’s common '\n",
    "                    'to use the word “gay” as an insult, or for someone to attack a '\n",
    "                    'commenter for being gay, but it is much rarer for the word gay to '\n",
    "                    'appear in a positive, affirming statements (e.g. “I am a proud gay man”). '\n",
    "                    'When the training data used to train machine learning models contain these '\n",
    "                    'comments, ML models adopt the biases that exist in these underlying distributions, '\n",
    "                    'picking up negative connotations as they go. When there’s insufficient diversity '\n",
    "                    'in the data, the models can over-generalize and make these kinds of errors.')\n",
    "            },\n",
    "           {\"description\": ('Imbalenced Data:\\n'\n",
    "                     'We developed new ways to balance the training '\n",
    "                     'data so that the model sees enough toxic and non-toxic examples '\n",
    "                     'containing identity terms in such a way that it can more effectively '\n",
    "                     'learn to distinguish toxic from non-toxic uses. You can learn more '\n",
    "                     'about this in our paper published at the AI, Ethics, and Society Conference.')\n",
    "            },\n",
    "        ]\n",
    "    },\n",
    "    \n",
    "  \"quantitative_analysis\": {\n",
    "    \"graphics\": {\n",
    "      \"description\": \" \"\n",
    "    }\n",
    "  },\n",
    "  \"schema_version\": \"0.0.1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d815585f",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcg = ModelCardGen.generate(data_sets=_data_paths,\n",
    "                            eval_config=_eval_config,\n",
    "                            model_path=_model_path, \n",
    "                            model_card=mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab58e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mcg"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
